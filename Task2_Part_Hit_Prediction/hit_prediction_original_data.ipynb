{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TASK-2: Hit Prediction (Classification)\n",
        "\n",
        "**Objective:** Predict whether a movie will be a Hit (0/1).\n",
        "\n",
        "**Dataset:** movies.csv (15 movies)\n",
        "\n",
        "**Approach:**\n",
        "1. Feature Selection (include cluster labels from Part A)\n",
        "2. Stratified Train-Test Split\n",
        "3. Model 1: Logistic Regression\n",
        "4. Model 2: Random Forest\n",
        "5. Cross-Validation for reliable evaluation\n",
        "6. Model Comparison and Selection\n",
        "7. Predict hit status for new movie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape: (15, 6)\n",
            "\n",
            "--- Dataset Preview ---\n",
            "   movie_id  avg_watch_time  completion_rate  ratings_count  avg_rating  hit\n",
            "0         1              45             0.60           1200         3.8    0\n",
            "1         2             110             0.90           8500         4.6    1\n",
            "2         3              60             0.65           2000         4.0    0\n",
            "3         4             130             0.95          12000         4.8    1\n",
            "4         5              40             0.55            900         3.6    0\n",
            "\n",
            "--- Target Distribution ---\n",
            "hit\n",
            "1    8\n",
            "0    7\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"../Dataset/original/movies.csv\")\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\n--- Dataset Preview ---\")\n",
        "print(df.head())\n",
        "print(\"\\n--- Target Distribution ---\")\n",
        "print(df['hit'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created Engagement & Popularity features (weighted)\n",
            "✅ Train-Test Split Done FIRST (Before any preprocessing)\n",
            "Training on 6 features: ['avg_watch_time', 'completion_rate', 'ratings_count', 'avg_rating', 'engagement', 'popularity']\n",
            "Training Set Size: 12\n",
            "Testing Set Size: 3\n",
            "\n",
            "Training Features:\n",
            "    avg_watch_time  completion_rate  ratings_count  avg_rating  engagement  \\\n",
            "0               45             0.60           1200         3.8    0.080867   \n",
            "10             115             0.91           9000         4.6    0.759514   \n",
            "12             125             0.93          11000         4.8    0.828224   \n",
            "2               60             0.65           2000         4.0    0.207188   \n",
            "5              120             0.92          10000         4.7    0.793869   \n",
            "\n",
            "    popularity  \n",
            "0     0.055138  \n",
            "10    0.545865  \n",
            "12    0.670593  \n",
            "2     0.130744  \n",
            "5     0.608229  \n"
          ]
        }
      ],
      "source": [
        "# Create Engagement & Popularity features FIRST\n",
        "# Objective: Group movies based on viewer engagement and popularity\n",
        "feature_cols = ['avg_watch_time', 'completion_rate', 'ratings_count', 'avg_rating']\n",
        "\n",
        "scaler_norm = MinMaxScaler()\n",
        "df_norm = pd.DataFrame(scaler_norm.fit_transform(df[feature_cols]), columns=feature_cols)\n",
        "\n",
        "# Create composite features (weighted)\n",
        "df['engagement'] = (df_norm['avg_watch_time'] + df_norm['completion_rate']) / 2\n",
        "df['popularity'] = 0.7 * df_norm['ratings_count'] + 0.3 * df_norm['avg_rating']\n",
        "\n",
        "# Use ALL 6 features: 4 original + engagement + popularity\n",
        "all_features = feature_cols + ['engagement', 'popularity']\n",
        "X = df[all_features].copy()\n",
        "y = df['hit']\n",
        "\n",
        "# Stratified Split FIRST - ensures both classes in train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"✅ Created Engagement & Popularity features (weighted)\")\n",
        "print(\"✅ Train-Test Split Done FIRST (Before any preprocessing)\")\n",
        "print(f\"Training on 6 features: {all_features}\")\n",
        "print(\"Training Set Size:\", len(X_train))\n",
        "print(\"Testing Set Size:\", len(X_test))\n",
        "print(\"\\nTraining Features:\")\n",
        "print(X_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Clustering Done on 6 Features (Fit on Train, Transform on Test)\n",
            "\n",
            "Training Cluster Distribution:\n",
            "1    6\n",
            "0    6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test Cluster Distribution:\n",
            "0    2\n",
            "1    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Features (7 total): ['avg_watch_time', 'completion_rate', 'ratings_count', 'avg_rating', 'engagement', 'popularity', 'cluster']\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Fit cluster scaler on TRAINING data only (using all 6 features)\n",
        "scaler_cluster = StandardScaler()\n",
        "X_train_cluster_scaled = scaler_cluster.fit_transform(X_train)\n",
        "\n",
        "# Fit KMeans on TRAINING data only\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "train_clusters = kmeans.fit_predict(X_train_cluster_scaled)\n",
        "\n",
        "# Add cluster labels to training set\n",
        "X_train = X_train.copy()\n",
        "X_train['cluster'] = train_clusters\n",
        "\n",
        "# Transform test data using FITTED scaler and kmeans (no fit!)\n",
        "X_test_cluster_scaled = scaler_cluster.transform(X_test)\n",
        "test_clusters = kmeans.predict(X_test_cluster_scaled)\n",
        "\n",
        "# Add cluster labels to test set\n",
        "X_test = X_test.copy()\n",
        "X_test['cluster'] = test_clusters\n",
        "\n",
        "print(\"✅ Clustering Done on 6 Features (Fit on Train, Transform on Test)\")\n",
        "print(\"\\nTraining Cluster Distribution:\")\n",
        "print(pd.Series(train_clusters).value_counts())\n",
        "print(\"\\nTest Cluster Distribution:\")\n",
        "print(pd.Series(test_clusters).value_counts())\n",
        "print(f\"\\nFeatures (7 total): {list(X_train.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Scaling Done Properly (Fit on Train, Transform on Test)\n",
            "\n",
            "Training Scaled Shape: (12, 7)\n",
            "Testing Scaled Shape: (3, 7)\n"
          ]
        }
      ],
      "source": [
        "# Scale features - Fit on TRAINING only, Transform both\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"✅ Scaling Done Properly (Fit on Train, Transform on Test)\")\n",
        "print(\"\\nTraining Scaled Shape:\", X_train_scaled.shape)\n",
        "print(\"Testing Scaled Shape:\", X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model 1 - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Logistic Regression Results ===\n",
            "Test Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [0 2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train Logistic Regression on properly scaled training data\n",
        "model_lr = LogisticRegression(random_state=42)\n",
        "model_lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on properly scaled test data\n",
        "y_pred_lr = model_lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"=== Logistic Regression Results ===\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model 2 - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Random Forest Results ===\n",
            "Test Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [0 2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train Random Forest on properly scaled training data\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on properly scaled test data\n",
        "y_pred_rf = model_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"=== Random Forest Results ===\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-Validation with Pipeline (Proper - No Leakage)\n",
        "\n",
        "**Key:** Use Pipeline to ensure scaling happens inside each CV fold, not before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Cross-Validation Results (5-Fold with Pipeline) ===\n",
            "\n",
            "NOTE: Using Pipeline ensures scaling is done inside each CV fold (no leakage)\n",
            "Logistic Regression:\n",
            "  CV Scores: [1. 1. 1. 1. 1.]\n",
            "  Mean CV Accuracy: 1.0000 (+/- 0.0000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Random Forest:\n",
            "  CV Scores: [1. 1. 1. 1. 1.]\n",
            "  Mean CV Accuracy: 1.0000 (+/- 0.0000)\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation with Pipeline - scaling happens INSIDE each fold\n",
        "# This is the proper way to do CV without data leakage\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"=== Cross-Validation Results (5-Fold with Pipeline) ===\\n\")\n",
        "print(\"NOTE: Using Pipeline ensures scaling is done inside each CV fold (no leakage)\")\n",
        "\n",
        "# Logistic Regression Pipeline\n",
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=42))\n",
        "])\n",
        "\n",
        "# Random Forest Pipeline\n",
        "pipeline_rf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# CV on TRAINING data only (not full dataset)\n",
        "cv_scores_lr = cross_val_score(pipeline_lr, X_train, y_train, cv=5)\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"  CV Scores: {cv_scores_lr}\")\n",
        "print(f\"  Mean CV Accuracy: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})\")\n",
        "\n",
        "cv_scores_rf = cross_val_score(pipeline_rf, X_train, y_train, cv=5)\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(f\"  CV Scores: {cv_scores_rf}\")\n",
        "print(f\"  Mean CV Accuracy: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std():.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparameter Tuning (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'classifier__C': 0.1}\n",
            "Best CV Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# GridSearch with Pipeline - proper way to tune hyperparameters\n",
        "# Note: When using Pipeline, prefix param name with step name\n",
        "\n",
        "param_grid = {'classifier__C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(pipeline_lr, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)  # Fit on TRAINING data only\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Score:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Comparison and Final Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MODEL COMPARISON ===\n",
            "\n",
            "                 Model  Test Accuracy  CV Mean Accuracy  CV Std\n",
            "0  Logistic Regression            1.0               1.0     0.0\n",
            "1        Random Forest            1.0               1.0     0.0\n",
            "\n",
            "=== FINAL MODEL SELECTION ===\n",
            "\n",
            "Based on the comparison:\n",
            "1. Logistic Regression is preferred for this small dataset because:\n",
            "   - Simpler model with less risk of overfitting\n",
            "   - More interpretable coefficients\n",
            "   - Works well with scaled features\n",
            "   - More stable with limited training data\n",
            "\n",
            "2. Random Forest may overfit on 15 samples with 100 trees.\n",
            "\n",
            "FINAL MODEL: Logistic Regression\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare Models\n",
        "print(\"=== MODEL COMPARISON ===\\n\")\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Test Accuracy': [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_rf)],\n",
        "    'CV Mean Accuracy': [cv_scores_lr.mean(), cv_scores_rf.mean()],\n",
        "    'CV Std': [cv_scores_lr.std(), cv_scores_rf.std()]\n",
        "})\n",
        "print(results)\n",
        "\n",
        "print(\"\\n=== FINAL MODEL SELECTION ===\")\n",
        "print(\"\"\"\n",
        "Based on the comparison:\n",
        "1. Logistic Regression is preferred for this small dataset because:\n",
        "   - Simpler model with less risk of overfitting\n",
        "   - More interpretable coefficients\n",
        "   - Works well with scaled features\n",
        "   - More stable with limited training data\n",
        "   \n",
        "2. Random Forest may overfit on 15 samples with 100 trees.\n",
        "\n",
        "FINAL MODEL: Logistic Regression\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict Hit Status for New Movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Final Model Evaluation on Test Set ===\n",
            "Test Accuracy: 1.0\n",
            "\n",
            "=== NEW MOVIE PREDICTION (6 Features + Cluster) ===\n",
            "Raw: watch_time=100, completion=0.88, ratings_count=7000, avg_rating=4.5\n",
            "Engagement: 0.656, Popularity (weighted): 0.443\n",
            "Cluster: 0\n",
            "\n",
            "Predicted Hit Status: HIT\n",
            "Prediction Probability: 71.70% chance of being a hit\n"
          ]
        }
      ],
      "source": [
        "# Use the best model from GridSearch (already trained on training data)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on held-out test set\n",
        "y_pred_final = best_model.predict(X_test)\n",
        "print(\"=== Final Model Evaluation on Test Set ===\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
        "\n",
        "# New movie prediction (6 features + cluster)\n",
        "new_movie_raw = pd.DataFrame({\n",
        "    'avg_watch_time': [100],\n",
        "    'completion_rate': [0.88],\n",
        "    'ratings_count': [7000],\n",
        "    'avg_rating': [4.5]\n",
        "})\n",
        "\n",
        "# Convert to engagement & popularity using the fitted scaler (weighted)\n",
        "new_movie_norm = scaler_norm.transform(new_movie_raw)\n",
        "new_engagement = (new_movie_norm[0][0] + new_movie_norm[0][1]) / 2\n",
        "new_popularity = 0.7 * new_movie_norm[0][2] + 0.3 * new_movie_norm[0][3]\n",
        "\n",
        "# Create full feature set (6 features)\n",
        "new_movie = pd.DataFrame({\n",
        "    'avg_watch_time': [100],\n",
        "    'completion_rate': [0.88],\n",
        "    'ratings_count': [7000],\n",
        "    'avg_rating': [4.5],\n",
        "    'engagement': [new_engagement],\n",
        "    'popularity': [new_popularity]\n",
        "})\n",
        "\n",
        "# Get cluster for new movie using fitted scaler and kmeans\n",
        "new_movie_cluster_scaled = scaler_cluster.transform(new_movie)\n",
        "new_movie_cluster = kmeans.predict(new_movie_cluster_scaled)\n",
        "new_movie['cluster'] = new_movie_cluster\n",
        "\n",
        "# Predict using the pipeline (it handles scaling internally)\n",
        "prediction = best_model.predict(new_movie)\n",
        "probability = best_model.predict_proba(new_movie)\n",
        "\n",
        "print(\"\\n=== NEW MOVIE PREDICTION (6 Features + Cluster) ===\")\n",
        "print(f\"Raw: watch_time=100, completion=0.88, ratings_count=7000, avg_rating=4.5\")\n",
        "print(f\"Engagement: {new_engagement:.3f}, Popularity (weighted): {new_popularity:.3f}\")\n",
        "print(f\"Cluster: {new_movie_cluster[0]}\")\n",
        "print(f\"\\nPredicted Hit Status: {'HIT' if prediction[0] == 1 else 'NOT HIT'}\")\n",
        "print(f\"Prediction Probability: {probability[0][1]:.2%} chance of being a hit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
